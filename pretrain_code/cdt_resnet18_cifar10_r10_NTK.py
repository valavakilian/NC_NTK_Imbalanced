# -*- coding: utf-8 -*-
"""CDT_resnet32_cifar10_R10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YipYh1Jhj2TSieteuJZG_5hUuGic0T3R
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# !pip install pytorch-ignite

import torch

import numpy as np
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torchvision.models as models

from tqdm import tqdm
from collections import OrderedDict
from scipy.sparse.linalg import svds
from torchvision import datasets, transforms

import pickle

from generate_cifar import IMBALANCECIFAR10
# from generate_mnist import IMBALANCEMNIST


import os
import sys
import pickle
import shutil
import gc

from sklearn import preprocessing
from torch.optim.lr_scheduler import ExponentialLR
# import resnet_cifar
# import ignite

gc.collect()

torch.cuda.empty_cache()


class CDTLoss(nn.Module):

    def __init__(self, delta_list, gamma=0.3, weight=None, reduction=None):
        super(CDTLoss, self).__init__()
        Delta_list = np.array(delta_list) ** gamma
        Delta_list = len(Delta_list) * Delta_list / sum(Delta_list)
        # Delta_list = Delta_list / np.min(Delta_list)
        print("Delta_list" + str(Delta_list))
        self.Delta_list = torch.cuda.FloatTensor(Delta_list)
        # self.Delta_list = torch.FloatTensor(Delta_list)
        self.weight = weight
        self.reduction = reduction

    def forward(self, x, target):
        # print("-"*20)
        # print("self.Delta_list: " + str(self.Delta_list))
        if self.reduction == "sum":
            output = x * self.Delta_list
            return F.cross_entropy(output, target, weight=self.weight, reduction='sum')
        else:
            output = x * self.Delta_list
            # print("x: " + str(x))
            # print("output: " + str(output))
            return F.cross_entropy(output, target, weight=self.weight)


class LDTLoss(nn.Module):

    def __init__(self, Delta_list, gamma = 0.3,weight=None, reduction = None):
        super(LDTLoss, self).__init__()
        self.gamma = gamma
        self.Delta_list = np.array(Delta_list) ** self.gamma
        self.Delta_list = len(self.Delta_list) * self.Delta_list / sum(self.Delta_list)
        self.Delta_list = torch.cuda.FloatTensor(self.Delta_list)
        self.weight = weight
        self.reduction = reduction

    def forward(self, x, target):
        if self.reduction == "sum":
            ldt_output = (x.T*self.Delta_list[target]).T
            return F.cross_entropy(ldt_output, target, weight=self.weight, reduction = 'sum')
        else:
            ldt_output = (x.T*self.Delta_list[target]).T
            return F.cross_entropy(ldt_output, target, weight=self.weight)

# ------- train fcn ---------------------------------------------------------------------------------------------------
def train(model, criterion, device, num_classes, train_loader, optimizer, epoch):
    model.train()

    cls_num_list_train = {}
    for c in range(0, C):
        cls_num_list_train[c] = 0

    per_class_acc = {}
    for c in range(0, num_classes):
        per_class_acc[c] = 0

    pbar = tqdm(total=len(train_loader), position=0, leave=True)
    for batch_idx, (data, target) in enumerate(train_loader, start=1):
        if data.shape[0] != batch_size:
            continue

        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        out = model(data)
        if str(criterion) == 'CrossEntropyLoss()':
            loss = criterion(out, target)
        elif str(criterion) == 'MSELoss()':
            loss = criterion(out, F.one_hot(target, num_classes=num_classes).float())
        elif str(criterion) == "CDTLoss()":
            loss = criterion(out, target)
        elif str(criterion) == "LDTLoss()":
            loss = criterion(out, target)

        predicted = torch.argmax(out, dim=1)
        loss.backward()
        optimizer.step()

        # accuracy = torch.mean((torch.argmax(out, dim=1) == target).float()).item()
        accuracy = torch.mean((predicted == target).float()).item()

        pbar.update(1)
        pbar.set_description(
            'Train\t\tEpoch: {} [{}/{} ({:.0f}%)] \t'
            'Batch Loss: {:.6f} \t'
            'Batch Accuracy: {:.6f}'.format(
                epoch,
                batch_idx,
                len(train_loader),
                100. * batch_idx / len(train_loader),
                loss.item(),
                accuracy))

        for label in target:
            cls_num_list_train[label.item()] += 1

        for c in range(0, num_classes):
            per_class_acc[c] += ((predicted == target) * (target == c)).sum().item()

        if debug and batch_idx > 20:
            break
    pbar.close()


    for c in range(0, num_classes):
        per_class_acc[c] /= cls_num_list_train[c]

    return per_class_acc


# ------- analysis fcn ------------------------------------------------------------------------------------------------
def analysis(graph, model, criterion_summed, device, num_classes, loader, NC_analysis=False):
    model.eval()

    N             = [0 for _ in range(C)]
    mean          = [0 for _ in range(C)]
    Sw            = 0
    Sw_maj = 0
    Sw_min = 0
    Sw_C = [0 for _ in range(C)]

    loss          = 0
    net_correct   = 0
    NCC_match_net = 0

    cls_num_list_train = {}
    for c in range(0, C):
        cls_num_list_train[c] = 0
    
    per_class_acc = {}
    for c in range(0, num_classes):
        per_class_acc[c] = 0

    with torch.no_grad():

        if NC_analysis:

            for computation in ['Mean', 'Cov']:
                pbar = tqdm(total=len(loader), position=0, leave=True)
                for batch_idx, (data, target) in enumerate(loader, start=1):
                    torch.cuda.empty_cache()
                    
                    data, target = data.to(device), target.to(device)

                    output = model(data)
                    h = features.value.data.view(data.shape[0],-1) # B CHW

                    predicted = torch.argmax(output, dim=1)
                    
                    for c in range(0, num_classes):
                        per_class_acc[c] += ((predicted == target) * (target == c)).sum().item()

                    for label in target:
                        cls_num_list_train[label.item()] += 1

                    # during calculation of class means, calculate loss
                    if computation == 'Mean':
                        if str(criterion_summed) == 'CrossEntropyLoss()':
                            loss += criterion_summed(output, target).item()
                        elif str(criterion_summed) == 'MSELoss()':
                            loss += criterion_summed(output, F.one_hot(target, num_classes=num_classes).float()).item()
                        elif str(criterion) == "CDTLoss()":
                            loss += criterion_summed(output, target).item()
                        elif str(criterion) == "LDTLoss()":
                            loss += criterion_summed(output, target).item()

                    for c in range(C):
                        # features belonging to class c
                        idxs = (target == c).nonzero(as_tuple=True)[0]

                        if len(idxs) == 0: # If no class-c in this batch
                            continue

                        h_c = h[idxs,:].double() # B CHW

                        if computation == 'Mean':
                            # update class means
                            mean[c] += torch.sum(h_c, dim=0) # CHW
                            N[c] += h_c.shape[0]

                        elif computation == 'Cov':
                            # update within-class cov

                            z = h_c - mean[c].unsqueeze(0) # B CHW

                            # for loop - for solving memory issue :((
                            for z_i in range(z.shape[0]):
                                temp = torch.matmul(z[z_i, :].reshape((-1, 1)), z[z_i, :].reshape((1, -1)))
                                Sw += temp
                                Sw_C[c] += temp

                            # cov = torch.matmul(z.unsqueeze(-1), # B CHW 1
                            #                    z.unsqueeze(1))  # B 1 CHW
                            # Sw += torch.sum(cov, dim=0)
                            #
                            # # Sw per class - for computing NC1 for majority and minority separately
                            # Sw_C[c] += torch.sum(cov, dim=0)

                            # during calculation of within-class covariance, calculate:
                            # 1) network's accuracy
                            net_pred = torch.argmax(output[idxs,:], dim=1)
                            net_correct += sum(net_pred == target[idxs]).item()

                            # 2) agreement between prediction and nearest class center
                            NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \
                                        for i in range(h_c.shape[0])])
                            NCC_pred = torch.argmin(NCC_scores, dim=1)
                            NCC_match_net += sum(NCC_pred==net_pred).item()

                    pbar.update(1)
                    pbar.set_description(
                        'Analysis {}\t'
                        'Epoch: {} [{}/{} ({:.0f}%)]'.format(
                            computation,
                            epoch,
                            batch_idx,
                            len(loader),
                            100. * batch_idx/ len(loader)))

                    if debug and batch_idx > 20:
                        break
                pbar.close()

                if computation == 'Mean':
                    for c in range(C):
                        mean[c] /= N[c]
                        M = torch.stack(mean).T
                    loss /= sum(N)
                elif computation == 'Cov':
                    Sw /= sum(N)

            graph.loss.append(loss)
            graph.accuracy.append(net_correct/sum(N))
            graph.NCC_mismatch.append(1-NCC_match_net/sum(N))

            for c in range(0, num_classes):
                per_class_acc[c] /= cls_num_list_train[c]
            graph.acc_perclass.append(per_class_acc)

            # loss with weight decay
            reg_loss = loss
            for param in model.parameters():
                reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()
            graph.reg_loss.append(reg_loss)

            # global mean
            muG = torch.mean(M, dim=1, keepdim=True) # CHW 1

            # between-class covariance
            M_ = M - muG
            Sb = torch.matmul(M_, M_.T) / C

            # avg norm
            W  = classifier.weight
            M_norms = torch.norm(M_,  dim=0)
            W_norms = torch.norm(W.T, dim=0)

            graph.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())
            graph.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())

            # tr{Sw Sb^-1}
            Sw = Sw.cpu().numpy()
            Sb = Sb.cpu().numpy()
            eigvec, eigval, _ = svds(Sb, k=C-1)
            inv_Sb = eigvec @ np.diag(eigval**(-1)) @ eigvec.T
            graph.Sw_invSb.append(np.trace(Sw @ inv_Sb))

            # ----------- new NC1
            graph.Sw_invSb2.append(np.trace(Sw) / np.trace(Sb))

            # Saving Sw, Sb, Sw_c
            # Sw_C = [Sw_C[c].cpu().numpy() for c in range(C)]
            # graph.Sw.append(Sw)
            # graph.Sb.append(Sb)
            # graph.Sw_C.append(Sw_C)

            # -------- NC1 on majority and minority classes -----------------------------------
            # 1- use global Sb for normalization --------------
            k2 = C//2
            for ii in range(k2):
                Sw_maj += Sw_C[ii]
                Sw_min += Sw_C[ii+k2]

            Sw_maj /= sum(N[:k2])
            Sw_min /= sum(N[k2:])

            Sw_maj = Sw_maj.cpu().numpy()
            Sw_min = Sw_min.cpu().numpy()
            graph.Sw_invSb_maj_global.append(np.trace(Sw_maj @ inv_Sb))
            graph.Sw_invSb_min_global.append(np.trace(Sw_min @ inv_Sb))

            graph.Sw_invSb_maj_global2.append(np.trace(Sw_maj) / np.trace(Sb))
            graph.Sw_invSb_min_global2.append(np.trace(Sw_min) / np.trace(Sb))

            # 2- compute Sb separately for majority and minority

            # a- use mean embeddings centered globally
            M_maj = M_[:, :k2]
            M_min = M_[:, k2:]
            Sb_min = torch.matmul(M_min, M_min.T) / k2
            Sb_maj = torch.matmul(M_maj, M_maj.T) / k2

            Sb_min = Sb_min.cpu().numpy()
            Sb_maj = Sb_maj.cpu().numpy()

            eigvec, eigval, _ = svds(Sb_min, k=k2) # rank of globally centered means is k2
            inv_Sb_min = eigvec @ np.diag(eigval ** (-1)) @ eigvec.T
            graph.Sw_invSb_min.append(np.trace(Sw_min @ inv_Sb_min))

            graph.Sw_invSb_min2.append(np.trace(Sw_min) / np.trace(Sb_min))

            eigvec, eigval, _ = svds(Sb_maj, k=k2)
            inv_Sb_maj = eigvec @ np.diag(eigval ** (-1)) @ eigvec.T
            graph.Sw_invSb_maj.append(np.trace(Sw_maj @ inv_Sb_maj))

            graph.Sw_invSb_maj2.append(np.trace(Sw_maj) / np.trace(Sb_maj))

            # b- use mean embeddings that are centered independently for majority and minority classes
            muG_maj = torch.mean(M[:, :k2], dim=1, keepdim=True)  # CHW 1
            muG_min = torch.mean(M[:, k2:], dim=1, keepdim=True)  # CHW 1

            M_maj = M[:, :k2] - muG_maj
            M_min = M[:, k2:] - muG_min
            Sb_min = torch.matmul(M_min, M_min.T) / k2
            Sb_maj = torch.matmul(M_maj, M_maj.T) / k2

            Sb_min = Sb_min.cpu().numpy()
            Sb_maj = Sb_maj.cpu().numpy()

            eigvec, eigval, _ = svds(Sb_min, k=k2-1)
            inv_Sb_min = eigvec @ np.diag(eigval ** (-1)) @ eigvec.T
            graph.Sw_invSb_min_local_cent.append(np.trace(Sw_min @ inv_Sb_min))

            graph.Sw_invSb_min_local_cent2.append(np.trace(Sw_min) / np.trace(Sb_min))

            eigvec, eigval, _ = svds(Sb_maj, k=k2-1)
            inv_Sb_maj = eigvec @ np.diag(eigval ** (-1)) @ eigvec.T
            graph.Sw_invSb_maj_local_cent.append(np.trace(Sw_maj @ inv_Sb_maj))

            graph.Sw_invSb_maj_local_cent2.append(np.trace(Sw_maj) / np.trace(Sb_maj))

            # -------------------------------------------------------------------------------------------

            # ||W^T - M_||
            normalized_M = M_ / torch.norm(M_, 'fro')
            normalized_W = W.T / torch.norm(W.T, 'fro')
            graph.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())

            # mutual coherence
            def coherence(V):
                G = V.T @ V
                G += torch.ones((C,C),device=device) / (C-1)
                G -= torch.diag(torch.diag(G))
                return torch.norm(G,1).item() / (C*(C-1))

            graph.cos_M.append(coherence(M_/M_norms))
            graph.cos_W.append(coherence(W.T/W_norms))

            graph.M.append(M.cpu().numpy())
            graph.W.append(W.cpu().numpy())
            graph.muG.append(muG.cpu().numpy())


R = 10
f = open("/scratch/st-cthrampo-1/vaalaa/NC_NTK/CDT_resnet18_cifar10_R" + str(R)+ "_NTK_Prints.txt", "w")
f.write("Create File!\n")
f.flush()


gamma_values = [0.0,0.25,0.5,0.75,1.0,1.25,-0.25,-0.5,-0.75,-1.0,-1.25]
# gamma_values = [-0.25, 0.25, 0.5, 0.75, 1.0, 1.25]
delta_list = [R, R, R, R, R, 1, 1, 1, 1, 1]

print("_" * 200)
print("Experiments with R = " + str(R))
print("Delta values = " + str(delta_list))
print("_" * 200)

num_version = 1
versions = [0]
f.write("Starting Experiments for cdt_gamma = " + str(gamma_values)  + " num_version = " + str(num_version) + " \n")
f.flush()

for version in versions:
    for gamma in gamma_values:
    

        f.write("cdt_gamma = " + str(gamma)  + " version = " + str(version) + " \n")
        f.flush()

        print("+" * 200)
        print("CDT Gamma = " + str(gamma))
        print("+" * 200)
        '''
        Edited the code to compute NC1 for majority and minority separately
        '''

        # ------ device -------------------------------------------------------------------------------------------------------
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print("Device: ", device)

        torch.set_default_dtype(torch.float32)

        # ------- parameters --------------------------------------------------------------------------------------------------
        debug = False  # Only runs 20 batches per epoch for debugging
        NC_analysis = True

        model_name = 'ResNet'    # 'VGG' or 'ResNet'
        dataset_name = 'CIFAR10' # 'CIFAR10' or 'MNIST'
        imb_type = 'step'

        root_path = "/scratch/st-cthrampo-1/vaalaa/NC_NTK/NC_CDT_resnet_CIFAR_R" + str(R) + "/cdt_gamma_" + str(gamma) + "_version_" + str(version)
        # root_path = "./here"
        
        experiment_complete_flag_file = root_path + "/ExpComplete.txt"
        print("save_log_path: " + str(root_path))
        if not os.path.exists(root_path):
            os.makedirs(root_path, exist_ok=True)
        elif not os.path.exists(experiment_complete_flag_file):
            shutil.rmtree(root_path)
            os.makedirs(root_path, exist_ok=True)
        else:
            print("Skipping this experiments, already done ...")
            continue


        rand_number = 1
        workers = 4
        train_sampler = None

        print("R: ", R)
        print(model_name)
        print(dataset_name)

        # number of samples
        maj_classes = [0, 1, 2, 3, 4]
        min_classes = [5, 6, 7, 8, 9]
        classes = maj_classes + min_classes
        N = 5000
        K = 10
        
        
        n_c_train_target = {}
        for c in range(K):
            if c in maj_classes:
                n_c_train_target[c] = N
            else:
                n_c_train_target[c] = int(N // R)
        

        print("n_c_target: " + str(n_c_train_target))
        N_train_total = sum(n_c_train_target.values())
        print("N_train_total: " + str(N_train_total))

        # ---------- dataset parameters
        if dataset_name == 'CIFAR10':
          im_size      = 32
        elif dataset_name == 'MNIST':
          im_size      = 28
        padded_im_size      = 32
        C                   = 10
        if dataset_name == 'CIFAR10':
            input_ch        = 3
        elif dataset_name == 'MNIST':
            input_ch        = 1

        # ------------ Optimization Criterion
        loss_name = 'CDT'

        # ------------- Optimization hyperparameters
        lr_decay            = 0.1

        # Best lr after hyperparameter tuning (MNIST)
        # if loss_name == 'CrossEntropyLoss':
        #    lr = 0.0679
        # elif loss_name == 'MSELoss':
        #   lr = 0.0184

        # CIFAR
        lr = 0.1

        epochs              = 350
        epochs_lr_decay     = [116, 232]

        batch_size          = 256

        momentum            = 0.9
        weight_decay        = 5e-4

        print("lr:", lr)
        print("lr_decay:", lr_decay)
        print("momentum:", momentum)
        print("weight_decay:", weight_decay)


        # ------------------- analysis parameters
        epoch_list = [1,   3,   5,   7,   9,
                      11,  20,  30,  40,  60,
                      80, 101, 120, 140, 160,
                      180, 201, 220, 235, 245, 250, 260,
                      275, 280, 290, 299, 305, 310, 315, 
                      320, 325, 330, 335, 340, 345, 349, 350]

        # -------------------- output parameters
        data_path = '/project/st-cthrampo-1/vala/data/'
        save_path = root_path




        # -------  model ------------------------------------------------------------------------------------------------------
        class features:
            pass


        def hook(self, input, output):
            features.value = input[0].clone()

        
        # model = models.resnet18(num_classes=C)
        # Small dataset filter size used by He et al. (2015)
        # model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False)
        # model.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)
        # turn bias of classifier off
        # model.linear = nn.Linear(in_features=512, out_features=10, bias=False)
       
        model = models.resnet18(pretrained=False, num_classes=C)
        model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False) # Small dataset filter size used by He et al. (2015)
        model.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)
        model.fc = nn.Linear(in_features=512, out_features=10, bias=False)
        model = model.to(device)
        classifier = model.fc
        classifier.register_forward_hook(hook)


        # -------  dataset NC paper -------------------------------------------------------------------------------------------
        # transform = transforms.Compose([transforms.Pad((padded_im_size - im_size)//2),
        #                                 transforms.ToTensor(),
        #                                 transforms.Normalize(0.1307,0.3081)])

        # train_loader = torch.utils.data.DataLoader(
        #     datasets.CIFAR10(data_path, train=True, transform=transform),
        #     batch_size=batch_size, shuffle=True)

        # analysis_loader = torch.utils.data.DataLoader(
        #     datasets.CIFAR10(data_path, train=True, transform=transform),
        #     batch_size=batch_size, shuffle=True)

        # test_loader = torch.utils.data.DataLoader(
        #     datasets.CIFAR10(data_path, train=False, transform=transform),
        #     batch_size=batch_size, shuffle=True)


        # -------  dataset imbalance ------------------------------------------------------------------------------------------

        transform_train = transforms.Compose([
                transforms.RandomCrop(32, padding=4),
                 transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            ])

        transform_val = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            ])

        train_dataset = IMBALANCECIFAR10(data_path, imb_type=imb_type,
                                        rand_number=rand_number, train=True, download=True,
                                        transform=transform_train, n_c_train_target=n_c_train_target, classes=classes)
        val_dataset = datasets.CIFAR10(data_path, train=False, download=True, transform=transform_val)
        
        n_c_test_target = [0 for _ in range(0,K)]
        for val_data in val_dataset:
            c = int(val_data[1])
            n_c_test_target[c] += 1
        
        print("^"*100)
        print("n_c_test_target: " + str(n_c_test_target))
        print("^"*100)


        cls_num_list = train_dataset.get_cls_num_list()
        cls_priors = [cls_num / sum(cls_num_list) for cls_num in cls_num_list]
        print('\nTotal number of samples: ', sum(cls_num_list))
        print('cls num list:')
        print(cls_num_list)


        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),
            num_workers=workers, pin_memory=True, sampler=train_sampler)

        analysis_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),
            num_workers=workers, pin_memory=True, sampler=train_sampler)

        test_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False,
            num_workers=workers, pin_memory=True)


        # -------  optimizer --------------------------------------------------------------------------------------------------
        if loss_name == 'CrossEntropyLoss':
            criterion = nn.CrossEntropyLoss()
            criterion_summed = nn.CrossEntropyLoss(reduction='sum')
        if loss_name == "CDT":
            criterion = CDTLoss(delta_list, gamma=gamma, weight=None, reduction=None)
            criterion_summed = CDTLoss(delta_list, gamma=gamma, weight=None, reduction="sum")
        if loss_name == "LDT":
            criterion = LDTLoss(delta_list, gamma=gamma, weight=None, reduction=None)
            criterion_summed = LDTLoss(delta_list, gamma=gamma, weight=None, reduction="sum")


        optimizer = optim.SGD(model.parameters(),
                              lr=lr,
                              momentum=momentum,
                              weight_decay=weight_decay)

        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,
                                                      milestones=epochs_lr_decay,
                                                      gamma=lr_decay)
        
        # scheduler = ignite.handlers.param_scheduler.create_lr_scheduler_with_warmup(lr_scheduler,
        #                                         warmup_start_value=0.0,
        #                                         warmup_end_value=lr,
        #                                         warmup_duration=5)


        # -------  analysis ---------------------------------------------------------------------------------------------------
        class graphs:
            def __init__(self):
                self.cur_epochs   = []
                self.accuracy     = []
                self.loss         = []
                self.reg_loss     = []
                self.acc_perclass = []

                # NC1
                self.Sw_invSb     = []
                self.Sw_invSb2    = []

                # NC2
                self.norm_M_CoV   = []
                self.norm_W_CoV   = []
                self.cos_M        = []
                self.cos_W        = []

                # NC3
                self.W_M_dist     = []

                # NC4
                self.NCC_mismatch = []

                self.M = []
                self.W = []
                self.N = []
                self.muG = []

                # NC1 - maj/min
                self.Sw_invSb_maj_global = []
                self.Sw_invSb_min_global = []
                self.Sw_invSb_min = []
                self.Sw_invSb_maj = []
                self.Sw_invSb_min_local_cent = []
                self.Sw_invSb_maj_local_cent = []


                self.Sw_invSb_maj_global2 = []
                self.Sw_invSb_min_global2 = []
                self.Sw_invSb_min2 = []
                self.Sw_invSb_maj2 = []
                self.Sw_invSb_min_local_cent2 = []
                self.Sw_invSb_maj_local_cent2 = []


        graph_train = graphs()
        graph_test = graphs()

        cur_epochs = []

        for epoch in range(1, epochs + 1):
            torch.cuda.empty_cache()

            per_class_acc = train(model, criterion, device, C, train_loader, optimizer, epoch)
            # graph.train_acc_perclass.append(per_class_acc)
            lr_scheduler.step()
            
            if epoch in epoch_list:
                cur_epochs.append(epoch)
                torch.cuda.empty_cache()
                analysis(graph_train, model, criterion_summed, device, C, analysis_loader, NC_analysis=NC_analysis)
                analysis(graph_test, model, criterion_summed, device, C, test_loader, NC_analysis=NC_analysis)


                graph_train.cur_epochs = cur_epochs
                graph_test.cur_epochs = cur_epochs

                graph_train.N = n_c_train_target
                graph_test.N = n_c_test_target

                path_train = save_path + 'graphs_train_save_{}_{}_R{}.pkl'.format(model_name, dataset_name, int(R))
                f_train = open(path_train, "wb")
                pickle.dump(graph_train, f_train)
                f_train.close()

                path_test = save_path + 'graphs_test_save_{}_{}_R{}.pkl'.format(model_name, dataset_name, int(R))
                f_test = open(path_test, "wb")
                pickle.dump(graph_test, f_test)
                f_test.close()

                print(f'Checkpoint saved. Epoch: {epoch} ')

                balanced_accuracy = sum([graph_train.acc_perclass[-1][c] for c in range(0,K)]) / K

                f.write("------------------------------\n")
                f.write("Epoch " + str(epoch) + " -> \n")
                f.write("reg_loss: " + str(graph_train.reg_loss[-1]) + " -> \n")
                f.write("train_acc_perclass: " + str(graph_train.acc_perclass[-1]) + " -> \n")
                f.write("test_acc_perclass: " + str(graph_test.acc_perclass[-1]) + " -> \n")
                f.write("Balanced Error: " + str(balanced_accuracy) + " -> \n")
                f.flush()
        
        torch.save(model,save_path + "model.pth")
          
        os.makedirs(experiment_complete_flag_file, exist_ok=True)

f.close()